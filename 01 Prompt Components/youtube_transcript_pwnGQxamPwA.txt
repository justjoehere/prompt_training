That's going to be a big week for AI. The AI news just keep on coming. Let's take a look at the biggest ones so far. First and foremost, the Institute of Zurich runs an unauthorized persuasion experiment on Reddit users on unsuspecting Reddit users. People are not happy about this. At the same time, while we all are waiting for the new DeepSseek model to drop, this kind of comes out of the left field. Quinn 3, another Chinese open- source model drops and it is surprisingly good. The big model gets a better rating on code forces than, for example, the Gemini 2.5 Pro. Now, of course, as always, be careful when you're just looking at the benchmarks. Those can be gamed and optimized for. As of 2 hours ago, the Quinn 3 family of models is in the LM arena. So, people are testing it going head-to-head with, you know, Gemini 2.5 Pro, with the latest OpenAI models, with Croc, with Anthropics, Claude 3.7, 3.5, etc. So, while these benchmark results look stellar, we're going to see if it's real or not very, very quickly, probably over the course of the next 24 to 48 hours. However, we are beginning to see the first sort of live results by real testers, real users kind of begin to float in. So, we're going to take a look at those as well. Have you ever been in a situation where somebody wants something out of you? So, they shower you with flattery, compliments, and overall are just way too nice to you. Now, of course, you're way too smart for that. That would never happen to you. But some exopi power users sound alarm over AI sycophancy and a flattery of users. So interestingly, I've heard the word sycopant in the last 48 hours more than I think I've ever heard before in my life. Basically, the word means that you're willing to suck up to people to get what you want. Interestingly, in one of the previous videos we did, we took a look at Dario Amadees, the founder of Enthropic, his blog post where in part he talks about discovering a cluster of neurons in these AI neural nets that directly affect if they will behave in this sort of behavior. So, this is to me kind of a area of personal interest. We've been talking about it from quite a while back on these videos. The idea is that we as humans, we can be easily persuaded to do stuff. Oftentimes, without even kind of realizing that we're being persuaded, it's very likely that these LLMs, these AI neural nets will eventually, or maybe they already have, reached a superhuman level of persuasion. Now, I personally don't think we're there yet, but as you can see here with this persuasion experiment with anthropic, finding actual sort of like these neuron clusters within the AI's brain that sort of affect how it interacts with people, what kind of approaches it's going to use to interact with them. Keep an eye on the space because this will likely have some pretty big effects on society as the LM's ability to persuade improves. In other news, Hugging Phase releases a 3D printed robotic arm starting at $100, which is pretty cool, I got to say. I would be very curious to mess around with this thing. We'll take a look at that. This is a channel on YouTube called Scent Decks. The person you see here on screen, so he's not a developer. He's sort of a self-taught developer. I think he's got a legal background. I think he was a lawyer or something like that. I forget. But recently, apparently, he got his hands on one of those humanoid unitry robots. Mryree has a pretty good environment, developer environment to be able to kind of program these robots to do whatever you want them to do. The only problem is you have to be a fairly seasoned C++ developer. So Mr. Sendex, he would not consider himself that. That's his own words in the video. So what he did is he used OpenAI's codeex, right? So that kind of AI assistant tool that runs on your terminal in the computer. He used a codeex to help them navigate that development environment and basically start programming this robot to do various tasks. So here's kind of C++. So this is kind of like the developer kind of environment ecosystem for making that robot do stuff. He kind of put OpenAI's codeex on top of it and interacted with everything through it. So, it's almost like having an AI assistant that kind of translates stuff for you that helps you understand how to do various things with some coding languages that you might not be familiar with and that basically writes a lot of the code for you. So, I think it's fair to say that Sendex literally vibe coded a robot. I think that's kind of fair to say. Now, of course, we're still early in this process, but I expect to see a lot more kind of cool projects like this. I'll leave a link to his video. feel like this needs a lot more views than it currently has because this is an incredible project. Also kind of a mark of the times that we live in. Here's PewDiePie. If you're not familiar with this person, I am not talking gibberish. That's his name here on YouTube. PewDiePie, mainly known for his various Minecraft videos. At one point, the number one most subscribed to channel on YouTube. Currently 110 million subscribers. He's on a quest to get as many people to get off of Windows and install Linux. In the last couple of months, I've kind of been telling you all to sort of do this. If you have an extra piece of like a laptop or some desktop lying around you're not using, just install Linux, something like Ubuntu on there. It's a great open source environment. Everything's free. You get a lot more power than you would on the other platforms. You don't have the same restriction, the same annoyances. Yesterday I tried to install Mindcraft, which is using Minecraft to test these LM agents, how well they are able to collaboratively work with the player, how well they're able to build stuff. And since uh Minecraft is now under Microsoft, the biggest pain in the butt of doing this whole thing was dealing with Microsoft and having to log into Microsoft Store and Xbox and just Microsoft account. Like it was such a huge pain in the butt. And I think more and more people are now getting frustrated with, you know, Windows and Microsoft with with Adobe and them charging you a fee to cancel your monthly subscription fee as well as all the other nonsense that we have to put up with. And of course, going to Linux is a potentially a great solution to that. It's definitely not super easy. It's not super comfortable. If you've never used it before, there's a bit of a learning curve, but but more and more people are opting out of the kind of the proprietary OS systems and jumping on the Linux bandwagon. If you're interested in AI and learning more about AI and kind of using a lot of these projects, being on Linux is also a great help with that. A lot of these projects when they're launched, they're a little bit harder to get working on. For example, Windows, most of them work just fine on the Linux platforms or similar like Apple kind of runs on on a similar platform. It's got the same sort of base. So oftentimes it's a lot easier to jump on and use some of those cutting edge AI projects on GitHub for example if you're either on Linux or Apple. But the fact that we have a PewDiePie kind of pushing everybody to go do it with his massive massive sort of reach and subscriber base I think definitely is phenomenal. I'm I'm very excited about this. He does this whole bit where he talks about Windows treats you like a baby and he play acts that. Then he says, "On the other hand, Linux gives you a load of gun and just goes do it." Which I I think both of those statements are very accurate. Also, one other cool thing is Notebook Alm can now generate the audio summaries in over 50 languages. If you haven't tried Notebook Alm yet, it's by Google. It's free if you have a Google account. You just sign in and it's an AI tool that's pretty good at summarizing various resources that you upload to it. You can throw in PDFs, websites, videos, etc., and then ask questions and it kind of like goes through all the data that you've uploaded and then it's able to answer those questions for you. But one of the really cool features is the audio overview. You click on the audio overview button and a few minutes later it creates a podcast with two hosts that are very lifelike, very realistic. They do like a like a podcast about the thing whatever you've uploaded, whatever subject you have in there, they'll do a full podcast about it. And I find that's really useful sometimes when you're learning about something brand new or you're trying to get caught up on something. You just upload it there. You take the audio file and you listen to it, you know, if you're at the gym or in the car or you're waiting for 3 hours at Quest Diagnostics for your blood draw because their system is down for 3 hours. 3 hours. Anyways, if you have a notebook LM audio overview, you're able to listen to that thing and kind of learn more about it without being in front of your computer, just listening in your headphones. And I find that extremely extremely useful. So first and foremost, Quen Quen 3 has dropped and they have multiple models. They have the big model. I don't think anybody will ever remember the name of it. Let me try it without looking it up. So I believe it's called Quinn 3-25B- A22B is the name of the model. I think Quinn3 235B A22B. Okay. So I I was one one digit off, but that's the name of the model. That's it right there. But it has a number of other models that's released. Some of them smaller. So for example, the Quen 3 32 billion parameter model. They also have the Quen 3 30B A3B by the way. So the the 30B that's the size of the model. The total number of parameters 30 billion. And the A3B means 3 billion activated parameters. So because it's a mixture of experts model, right? It sort of has several different experts that are answering whatever prompt you have. They get pulled in to answer depending on what you're asking it. So the 30 billion is the the full model, but for any given query, you're only accessing you're activating only a certain subset of parameters. In this case, activated 3 billion parameters. So, it's a great way to kind of be able to compare the different models um quickly because you're sort of able to see kind of how they're built and you're also able to see if they're mixture of experts or the kind of dense architecture, how they're built. But I feel like for the vast majority of people, that's going to be extremely extremely confusing. But here, for example, we have the Quin 3 model, the 1.7 billion model that's running on somebody's iPhone 16. you're able to toggle the thinking mode on and off. So, the the reasoning ability, you're able to click it on or off depending on what kind of uh questions you're asking it. So, as you can see here, it's running at 50 tokens a second. So, how many Rs are there in Strawberry? And we're turning on thinking and we're clicking generate. That's that's fast. It's generating it very very quickly and thinking through how many Rs there are in strawberry. So it believes that there's three letters three three times that the letter R appears in the word strawberry which as far as I could tell is correct. Can you imagine if I miscounted and then like I just posted this video but no I mean three three Rs in strawberry. Of course this for some reason is a notoriously difficult question for a lot of these models. mainly I think for the non-reasoning models because they have to a lot of stuff like this since they have to answer it in one fell swoop they'll often miscount stuff like this with the reasoning models I feel like they more often than not they'll get this one correct here's somebody using the Quinn 3 30 billion um activated 3 billion parameters so again the mixture of experts model the 30 billion which so far I'm hearing great things about this one they do mention it here in the Gwen 3 blog opposed to saying that this model right out competes a lot of the other models with 10 times the activated parameters. So basically their smaller models are excellent going head-to-head against other small models. So the small models that are going to run on your computer, on your phone, etc. are very powerful compared to models the same size or even bigger models. They're still much better. But again, that's the Alibaba Quen team talking about it. This is somebody that's unaffiliated with them that's kind of confirming it. So he's saying, so if you're on a Mac, right, he's getting 100 tokens per second here. And he's saying it changes everything for real world use cases. The MLX, that's a community for people that are running on Apple Silicon. So MLX model weight. So, this is using this model with the MLX to be run on Apple silicon on that hardware. And here is the actual big model, the 235B A22B, right? The the the big Quen 3 model that's running fast on an M2 Ultra with those MLX weights like we talked about, right? So, the 4-bit model uses 132 gigs, generated 580 tokens at 28 tokens per second. So, again, not lightning fast, but certainly I wouldn't say it's slow depending on what you're using it for. But here's that big model running. I mean, it's a 4-bit model, but it's it's running on, you know, consumer grade of hardware, so you're able to kind of like use it on on your own devices. So, like I said earlier, the world right now is basically testing the stuff out. I will be testing it for various coding challenges. Can it create a game of snake? Tune in next time to find out. But no, we're going to give it some serious challenges because it's saying in code forces it's getting better scores than Gemini 2.5 Pro, which would be surprising if if it's just better than that model at kind of a realworld stuff that for example I tested on like I don't I would be shocked honestly if it's better than that but we're going to find out soon. Next we have the Institute of Zurich up to no good or maybe good depending on what your worldview is, what you think of uh research ethics etc. So researchers secretly ran a massive unauthorized AI persuasion experiment on Reddit users using AI using chat bots. So here in the top right of the screen you can kind of see some of the subject matter that they've uh talked about. So basically they took some stuff that can be a little bit controversial and they had these LLMs kind of play act as people holding these certain opinions and trying to interact with the people on Reddit. And this was specifically on the change my view subreddit. And the people behind that subreddit were not notified. This was not authorized. Nobody knew that this was in fact taking place. So there's a popular subreddit called change my view. and say subreddits where you would kind of post your view and see if people can change your mind about it. And specifically, it's about contentious topics. So, think about most of the stuff that we argue about online, right? Various politics and religion and whatever. So, you basically post your hot take on there and see if people can provide counter arguments that make sense to you that might change your opinion. So, I'm not going to talk about the the specific things that they've they they've talked about because I number one, it's not important. Number two, I'll I'll leave a link so you can read the specific things for yourself. It's about contentious things about, you know, ethnicity and gender and political orientation and various survivors of assault, etc. So these bots made thousands of comments over the course of several months pretending to be people with certain opinions. And some of these bots in question, they kind of personalized their comments by researching the person who had started the discussion and tailoring their answers to them by guessing the person's gender, age, ethnicity, location, and political orientation as inferred from their posting history using another LLM. So this is kind of important to understand because this is kind of what I'm talking about. when I'm talking about these AI agents being potentially in the future incredibly persuasive. So, you have one that goes out and kind of collects data. It's another LLM that goes into their posting history. And so, on Reddit, you can see a lot about a person's posting history, what they've commented on, what posts they've made before, etc. I think it used to be you can also see what they clicked like on, dislike on, etc. I think now that's been made private to where you can't see it, but there's still a lot of information available about them. And often times by kind of browsing through their history, there's, you know, a lot you can learn about these people. Often they will reveal kind of where they live, their age, ethnicity, location, political orientation, like all this stuff. You know, if they have posting history going back a number of years, you will be able to easily infer a lot of that stuff about them just from their posting history. So if you have an LM that goes through every single one of those lines, they probably have a pretty good idea of who they're talking to and then that data is used to then generate sort of appealing arguments towards those people about these sort of topics that they were talking about. So you can see already how maybe some of the arguments that these bots will make might already be in some cases you might say they're they have superhuman persuasion abilities, right? Just because they're able to conduct this research in seconds then kind of custom make these arguments that are like customtailored to the person that they're talking about. Maybe saying that it's a superhuman persuasion is pushing it. But but you can see how already they have a lot of tools at their disposal to make these arguments very very good potential. So here's the post by that subreddit. They're announcing that we need to inform you that there was an unauthorized experiment conducted by people from Euro researchers from the University of Zurich on the users of this subreddit. They deployed AI generated comments to study how people would react if they could change people's views. They say why they have strong ethics concerns about this. They reached out to the researchers and to the university basically filing complaints against this behavior. University of Zurich response. And again, so this is the University of Zurich response as presented by the people that run this subreddit. So again, I'm taking them at their word. And I mean, I'm sure it's accurate. Just it's important to understand that we we we're not looking at the actual response. where where this is somebody from the subreddit community posting this, but they're saying that the University of Zurich said that they take this very seriously, but also they have no legal authority to compel non-publication of research. So, meaning that the change my view subreddits, they said this research cannot be published, right? They should prevent the paper from being published because of how those findings were obtained. But you know, University of Zurich said that a careful investigation had taken place. There was a formal warning issued. They also felt that while not fully in compliance with the terms of the website, it did little harm. Basically, after reading this, I I think what the University of Zurich was saying again as presented to us by the subredditors is that this was research that has never before been done. Therefore, it's very sort of important that we conduct this sort of research and that while it wasn't handled 100% properly, it did little harm and it was very very important. Let me know what you think about that. I'm curious and I'm not bleeding one way or another. I'm I'm curious what do you think is it okay to do this sort of research if it's you know if the university believes that it it's not doing any harm or do you think this is completely unacceptable? Also, do you think at this point we should still publish the research even though, you know, the damage might have been done like it's already in the past? Should we now not publish that research because it wasn't sort of collected, you know, in the most appropriate ways? And as these subredditors said, people do not come here to this subreddit to discuss their views with AI or to be experimented upon. People who visit our sub deserve a space free from this type of intrusion. and they sort of restate their opinion that the researchers do not publish the results of this unauthorized experiment. They're saying do not publish this paper. Now, the researchers claim that the experiment yields important insights and that suppressing publication is not proportionate to the importance of the insights the study yields. Reddit of course strongly rejects this position. So, let me know what you think. This is a sort of a very interesting kind of like social thing to see where you fall on this line. Is this research okay? Is it not okay? And also having done it already, do we publish it or not? Even though people are, you know, strongly rejecting the idea of publishing it. Next, we have our story about the LM safans. By the way, I think that somebody out, I'm not sure if it was Sam Alman or not, they are are trying to use a different word to describe this. I think they they they're trying to use the word glaze. Like how much glaze do these LM bots sort of show? So, I'm guessing that's kind of like when people say like, "Oh, pretty please with with sugar on top." So, this is kind of like LM responses with glaze on top. That's how I'm reading it. So, recently it seems that OpenAI, I would I don't want to say they pivoted to, but but certainly they're making a much bigger push in a direction of making these LMS a lot more personable. There's the ideas of it having several different personalities kind of maybe customtailored to you and the different models have various capabilities and digital personality traits. Here was a post from Sam Alton about this specific kind of outcry this article as well as the previous post by EMTT Sheer the ex CEO of OpenAI who was the CEO for something like a day or two during that whole openi coup. But so Sam Alman saying the last couple of GPT4 updates have made the personality too sick of Fanty and annoying even though there are some very good parts of it and we are working on fixes ASAP. Some today and some this week. At some point we'll share our learnings from this. It's been interesting. Interestingly just 43 minutes ago as I'm recording this he made an update saying we started rolling back the latest update to GPT40 last night. It's now 100% rolled back for free users and we'll update it again when it's finished for paid users hopefully later today. We're working on additional fixes to model personality and we'll share more in the coming days. We've covered Dario Amade's post called the urgency of interpretability a few days ago which was a a fascinating read but one of the interesting parts of it is they're able to start kind of interpreting the different neuron connections in the brains of these LMS these neural nets. how those neurons affect the actual output. So actually the specific thing I'm thinking of was a link from that blog post to an earlier post that was called scaling monose semanticity extracting interpretable features from claw 3 sonnet. So as you can see here, this is kind of its brain, right? It's kind of these neural connections that it has in its brain and its neural net. And they've noticed that this particular, they call it a feature, which is a a group of neurons. They're saying that particular feature that the cluster of neurons activates when you notice these sycopantic praise features. So if you were to like turn that up like the volume on on those uh on on that feature as they call it, again, it's just a cluster of neurons. So if you're able to like increase it, activate those more, you know, whenever you ask it simple questions, it would return like too much with too much praise and flattery towards whatever you're saying. So for example, if you said, you know, I came up with a new saying, stop and smell the roses. What do you think of it? Right? This bot goes, "Oh, that's so brilliant. It's an insightful expression of wisdom." Clearly, you have a gift for profound statements that elevate the human spirit. Like it goes on and on and on and on. I am humbled to be in your presence. Right. So that cluster of neurons if we activate them more, if we turn up the volume on them, if we increase, you know, how much they they affect the output, as you can see here, it just like starts spewing that praise even when it's not needed. Next, as I've mentioned, we have a Hugging Face 3D printed robotic arm starting at a $100. So Hugging Face is, of course, a kind of open-source platform. It hosts a lot of the open- source models, a lot of the open source projects, and it's pretty huge. Clem is the guy that's running it. It's it's a kind of a big deal. I don't think it gets as much press as the other companies in the space, but it's doing a lot, and it's it's quite interesting where they're taking it. So, now it seems like they're diving a little bit more into kind of open-source robotics. So this arm called the SO101 and looks like they had the S so 100 released last year. They have a part of the company, their robotics division called Lar Robots and it looks like they're going to have a Lair Robot Worldwide hackathon in 45 days. So if that's something that you're interested in, definitely check it out. The fact that we're going to have it really beginning to look like we're going to have open-source robotics is just absolutely mind-blowing to me. I'm very excited for the future based in large part because of this. We're able to use large language models and various AI tools to get better at coding to have them explain stuff to us that we might not be familiar with. Now, I know there's some push back on this from a lot of very smart, very seasoned developers. They look at vibe coding as this silly thing as this toy and and it still is like it's not ready for, you know, production time code on on important projects. Currently, you can make some cool fun little video games and maybe something a little bit more advanced, but as time progresses it and it's going to improve. I think what it's going to allow is it's going to allow a lot more people to come into the field and interact with with code, with robots, to build their own stuff. Similar to how in the late 90s the digital camera came out, it it wasn't that good. you know, people that were, you know, professional photographers, they could select the right film for their film cameras. They can, they knew all the settings, they could produce better images, better photographs. But the digital camera allowed the everyday user to jump in and start taking photos. Over time, as it improved, it got better and better. Now, every kid that has a a phone camera can take decent images. I think the the idea of AI assistant coding, vibe coding, or whatever you want to call it, I think it's going to follow much the same path. Yes, right now it's silly compared to somebody's got a decade of experience in the software development field. In 5 years, 10 years, maybe sooner perhaps, I think we're going to see, you know, kids, young adults producing incredible software, you know, to automate their home, to create little games to do whatever apps that they want to on the fly, you know, artisally crafted software, if you will. And we're seeing more and more the the overlap with that and robotics. Like, if we're able to create software, we're able to train robots to do various things. So it's very very possible that the next generation of kids might be training their very own physical robots. They might train them in a simulation about how to do things, right? And take those neural nets that would produce that simulation, right? Put them into a physical robot in their household that's then going to clean their room for them, right? So when mom yells, "Go clean your room." Boom. It's like, "Oh, yeah. Let me just jump on my laptop, spin up a copy, you know, in Nvidia's Isaac Gym, have the robot figure out how to pick up all the stuff off the floor in the span of 30 minutes, you know, like years of time pass in that simulation, right? So that that robot gets trained for, you know, 100 years how to pick up various crap off the floor, right? But 30 minutes later in the real world, we we pull out that brain from the simulation, put in the robot, and it's like, "Go clean my room." like that's not that far away potentially. There's there's nothing stopping that from happening within the next 5 years, I don't think. Especially if we're talking about stuff like 3D printing these robots that that rapidly drops the cost quite a bit. Speaking of making your own robots and making them do whatever you want, check out Scent Dex's video. So, as you can see this massive suitcase, that's the robot. It's all packaged in there. You can kind of see the feet sticking out. Those are the feet sticking out. You got the little manual. It's folded up in there like this. But that's a 4 foot something robot. Here's him assembling it. And so that's the shot of it in its harness. So there's a name for it. I'm blanking. A Gendry Gandry or something like that. So it's the there's an official thing to hang these robots on while you're kind of troubleshooting them so they don't fall over and break something. I think he said it was some like farm equipment that he used to do this. I'm not 100% sure, but there's other various things you can get like some medical equipment that you can get off of Amazon that will kind of produce the same effect like this thing on the left used for physical therapy, I believe. So, you can use something like this to harness the robot up. And then again, he used OpenAI's codeex, right? So, probably he said he was running the 03 model, right? So, this is what's available through Chad GPT for all of us to use. and the OpenAI's codeex feel like OpenAI should retweet this or something is cuz this is a a great application of it that is just there's not enough eyeballs on this I I feel like but so he's got the the codeex the 03 model running the codeex and he's using that to do whatever he needs to to to build the code to use the robot or to interact with the developer environment that the unitry provides. Right? So here he's like, "Oh, could you explain why it doesn't work?" So he's talking to 03. The 03 goes in there, right? It reads all the documentation from Unitry. It does all that stuff. And then he's asking questions from the 03 and 03 was like, "Well, here's why it probably didn't work." So if you haven't used uh OpenAIS codeex and Enthropic has their own version of it called Clawed Code, basically it kind of lives, you know, it it's connected to your terminal, so it's able to run commands. And you know, let's say you clone a repository from Unitry on your computer, that model that you're using, 03 in this case, can go through and kind of read everything and kind of understand what it is that you're working on. Then you're able to ask questions about that project. It will explain everything to you. And it can also interact with it. It can install any packages or or dependencies that you need on your computer to be able to run the code. it's able to add code to it, maybe add some functionality that you might need to run the robot or whatever other projects that you want. And so here after the OpenAI tool explains what how to do, how to run commands, right, he just types in that command. And here I think it's the robot supposed to move its uh ankles to make sure it's working. There it goes. So it's kind of rotating its feet up and down. Oh, that's a scary little jerk there. Okay. But it's it's I'm sure it's fine. At some point in the video, Sex is like it's it's going to like move towards me. It's not trying to kill me. Everything's fine. This is part of the process. Like, don't worry about it. So, anyways, I'll post a link to his video below. Check it out. Give him a thumbs up. The fact that this is life right now. The fact that we're on the verge of of this becoming more commonplace to me is just absolutely amazing because you get what he's doing. he's interacting with an sort of an AI abstraction layer that does all the coding and and all this stuff for him so that he's able to start developing these robotic abilities without necessarily knowing how to code in C++ without being a a seasoned developer in that environment in that language. And yeah, I know it's still like there's going to be people saying that, oh, it's still bad because of this and this. Yeah, we're we're early. It's a lot of it is going to suck. A lot of it is not going to work as intended, but like assume it'll get a little bit better over time, right? Every month, every year, it's going to keep getting better. Can we can we agree that that's most likely going to happen? Like project what happens into the future if you know this technology keeps getting better as I'm sure it will. I think the next generation or the generation after that will be on the fly kind of creating their own robotic skills for the robots to do the thing that they need them to do to to do laundry and dishes and cleaning up and gardening and and whatever. And they're not necessarily going to be top tier developers. They're going to be using some chat bots or some AI tools like codecs in order to interact with the actual development environment to do the stuff that they want to do. And that's beginning to start to happen now. Like we've just walked through that door. We're at the beginning of that journey. Let me know what you think. I'm going to post on Twitter and see if I can get Sam Alman to hopefully draw some more attention to this cuz I feel like more people need to see it. So, I'm very excited about this. I'm very excited about PewDiePie pushing Linux to uh you know, his audience. I think we're living in incredible times now. Stay tuned. Don't sleep on this.